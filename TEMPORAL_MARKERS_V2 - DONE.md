# RumiAI Output Backward Compatibility Implementation

## Overview
This document outlines the implementation for making RumiAI's output backward compatible with the legacy 3-file structure while maintaining the Python-only processing pipeline. This applies to all 8 insight types generated by the system.

## File Structure Requirements

### All 8 Insights Requiring 3-File Structure
1. **creative_density** - Creative content density analysis
2. **emotional_journey** - Emotional arc and intensity tracking
3. **person_framing** - Human presence and framing analysis
4. **scene_pacing** - Scene changes and pacing metrics
5. **speech_analysis** - Speech patterns and rhythm analysis
6. **visual_overlay_analysis** - Text overlays and visual elements
7. **metadata_analysis** - Video metadata insights
8. **temporal_markers** - Beginning/end engagement markers

### Legacy 3-File Output (Per Insight)
Each insight must generate 3 files:

1. **`{insight_type}_complete_{timestamp}.json`**
   - Full wrapper with metadata
   - Contains prompt_type, success, response, parsed_response
   
2. **`{insight_type}_ml_{timestamp}.json`**
   - Clean JSON without prefixes
   - Uses generic keys: CoreMetrics, Dynamics, Interactions, etc.
   - Most suitable for ML model training with standardized schema
   
3. **`{insight_type}_result_{timestamp}.json`**
   - JSON with prefixes (using proper .json extension for ML compatibility)
   - Uses prefixed keys: densityCoreMetrics, densityDynamics, etc.
   - Best for feature extraction when combining multiple insights
   - **Changed from .txt to .json for better ML tooling support**

## Metadata Structure for _complete File

Since we're using precompute functions (not Claude API), the metadata structure will be:

```json
{
  "prompt_type": "creative_density",     // Function name for backward compatibility
  "success": true,                        // True if function completed without exception
  "response": "{ stringified JSON }",     // String version of prefixed result
  "parsed_response": {                    // Unprefixed ML format
    "CoreMetrics": { ... },
    "Dynamics": { ... }
  }
}
```

### Field Definitions:
- **prompt_type**: Use the function name (e.g., "creative_density") to maintain compatibility with systems expecting this field
- **success**: Set to true when precompute function returns valid data
- **response**: Stringified JSON of the prefixed format (what goes in _result file)
- **parsed_response**: Unprefixed ML format (what goes in _ml file)

## File Generation Rules

### Timestamp Synchronization
- All 3 files for a single insight MUST share the same timestamp suffix
- Generate timestamp once at the beginning of save operation
- Example for creative_density at 2025-08-11 13:13:06:
  - `creative_density_complete_20250811_131306.json`
  - `creative_density_ml_20250811_131306.json`
  - `creative_density_result_20250811_131306.json`
- This ensures file grouping and atomic operations

### Error Handling Strategy
- **Fail Fast**: If any file generation or format conversion fails, the entire insight save operation must fail
- Do NOT save partial files (all 3 or none)
- Raise exception immediately on conversion errors
- Ensure atomic operation: either all 3 files are saved successfully or none are saved
- Log the specific conversion/save error before failing
- This prevents data inconsistency and makes debugging easier

## Implementation Status

### Current State
- ✅ URL flow saves single file per insight
- ✅ Video ID flow saves single file per insight
- ✅ Temporal markers working for both flows
- ❌ Missing 3-file structure for backward compatibility

### Required Changes
1. Modify `save_analysis_result()` method to generate all 3 files
2. Convert between prefixed and unprefixed formats
3. Ensure same timestamp for all 3 files per insight
4. **Apply to BOTH flows**:
   - URL flow (process_video_url): Full video processing pipeline
   - Video ID flow (process_video_id): Gap-filler for existing analysis
   - Both flows must produce identical 3-file structure for consistency

## Format Conversion Specification

### Prefix Mapping Rules
Convert between prefixed (RESULT format) and unprefixed (ML format) using these rules:

| Insight Type | Prefixed Keys (RESULT) | Unprefixed Keys (ML) |
|-------------|------------------------|---------------------|
| creative_density | densityCoreMetrics, densityDynamics, densityInteractions, densityKeyEvents, densityPatterns, densityQuality | CoreMetrics, Dynamics, Interactions, KeyEvents, Patterns, Quality |
| emotional_journey | emotionalCoreMetrics, emotionalDynamics, emotionalInteractions, emotionalKeyEvents, emotionalPatterns, emotionalQuality | CoreMetrics, Dynamics, Interactions, KeyEvents, Patterns, Quality |
| person_framing | personFramingCoreMetrics, personFramingDynamics, personFramingInteractions, personFramingKeyEvents, personFramingPatterns, personFramingQuality | CoreMetrics, Dynamics, Interactions, KeyEvents, Patterns, Quality |
| scene_pacing | scenePacingCoreMetrics, scenePacingDynamics, scenePacingInteractions, scenePacingKeyEvents, scenePacingPatterns, scenePacingQuality | CoreMetrics, Dynamics, Interactions, KeyEvents, Patterns, Quality |
| speech_analysis | speechCoreMetrics, speechDynamics, speechInteractions, speechKeyEvents, speechPatterns, speechQuality | CoreMetrics, Dynamics, Interactions, KeyEvents, Patterns, Quality |
| visual_overlay_analysis | visualOverlayCoreMetrics, visualOverlayDynamics, visualOverlayInteractions, visualOverlayKeyEvents, visualOverlayPatterns, visualOverlayQuality | CoreMetrics, Dynamics, Interactions, KeyEvents, Patterns, Quality |
| metadata_analysis | metadataCoreMetrics, metadataDynamics, metadataInteractions, metadataKeyEvents, metadataPatterns, metadataQuality | CoreMetrics, Dynamics, Interactions, KeyEvents, Patterns, Quality |
| temporal_markers | markers, video_id, analysis_timestamp | markers, video_id, analysis_timestamp |

**Note**: temporal_markers has a different structure and doesn't follow the 6-block pattern.

### Conversion Algorithm
```python
def convert_to_ml_format(prefixed_data, insight_type):
    """Convert prefixed format to ML format by removing type prefix"""
    if insight_type == 'temporal_markers':
        return prefixed_data  # No conversion needed
    
    ml_data = {}
    prefix = get_prefix_for_type(insight_type)  # e.g., "density", "emotional"
    
    for key, value in prefixed_data.items():
        if key.startswith(prefix):
            # Remove prefix and capitalize first letter
            new_key = key[len(prefix):]
            new_key = new_key[0].upper() + new_key[1:] if new_key else key
            ml_data[new_key] = value
        else:
            ml_data[key] = value
    
    return ml_data
```

## Implementation Code Example

### Updated save_analysis_result Method
```python
def save_analysis_result(self, video_id: str, analysis_type: str, data: dict) -> Path:
    """Save analysis result in 3-file backward compatible format."""
    from datetime import datetime
    import json
    
    # Generate single timestamp for all 3 files
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # Create directory structure
    analysis_dir = self.insights_handler.get_path(video_id, analysis_type)
    self.insights_handler.ensure_dir(analysis_dir)
    
    try:
        # 1. Prepare the three formats
        result_data = data  # Already in prefixed format from precompute
        ml_data = convert_to_ml_format(data, analysis_type)
        
        # 2. Create _complete file content
        complete_data = {
            "prompt_type": analysis_type,
            "success": True,
            "response": json.dumps(result_data),
            "parsed_response": ml_data
        }
        
        # 3. Generate file paths with same timestamp
        complete_path = analysis_dir / f"{analysis_type}_complete_{timestamp}.json"
        ml_path = analysis_dir / f"{analysis_type}_ml_{timestamp}.json"
        result_path = analysis_dir / f"{analysis_type}_result_{timestamp}.json"
        
        # 4. Save all 3 files atomically (fail fast on any error)
        self.insights_handler.save_json(complete_path, complete_data)
        self.insights_handler.save_json(ml_path, ml_data)
        self.insights_handler.save_json(result_path, result_data)
        
        logger.info(f"Saved 3-file set for {analysis_type} to {analysis_dir}")
        return complete_path
        
    except Exception as e:
        # Fail fast - log and re-raise
        logger.error(f"Failed to save {analysis_type} for {video_id}: {e}")
        # Clean up any partial files if they exist
        for path in [complete_path, ml_path, result_path]:
            if path.exists():
                path.unlink()
        raise
```

### Helper Function for Prefix Mapping
```python
def get_prefix_for_type(insight_type: str) -> str:
    """Get the prefix used in RESULT format for each insight type."""
    prefix_map = {
        'creative_density': 'density',
        'emotional_journey': 'emotional',
        'person_framing': 'personFraming',
        'scene_pacing': 'scenePacing', 
        'speech_analysis': 'speech',
        'visual_overlay_analysis': 'visualOverlay',
        'metadata_analysis': 'metadata',
        'temporal_markers': None  # No prefix needed
    }
    return prefix_map.get(insight_type, insight_type)
```

## Next Steps
1. Implement the updated save_analysis_result() method in rumiai_runner.py
2. Add the format conversion functions
3. Deploy and verify all 8 insights generate proper 3-file structure
4. Verify backward compatibility with existing systems