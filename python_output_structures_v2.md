# Python-Only Pipeline Output Structures - VERIFIED VERSION

This document details the **ACTUAL** JSON output structure for each of the 7 ML analysis flows produced by the Python-only processing pipeline in RumiAI, verified against real implementation and output files.

**Version**: 3.0.0 - Verified Against Implementation  
**Last Updated**: 2025-01-13  
**Processing Mode**: Python precompute functions with $0.00 cost  
**Verification Status**: ✅ Cross-checked with actual code and output files

## Critical Discrepancies Found

The previous documentation had several field name mismatches. This version reflects the **ACTUAL** implementation:

### Key Differences:
1. **Visual Overlay**: Uses `totalOverlays` not `totalTextOverlays` as primary field
2. **Field names**: Several camelCase vs snake_case inconsistencies fixed
3. **Nested structures**: Some documented nested objects are actually flat
4. **Missing fields**: Several fields documented but not actually implemented

## Overview

Each ML flow outputs a standardized 6-block CoreBlocks structure generated by Python functions:
1. **CoreMetrics** - Basic measurements and counts
2. **Dynamics** - Temporal changes and progressions  
3. **Interactions** - Element relationships and synchronization
4. **KeyEvents** - Specific moments and occurrences
5. **Patterns** - Recurring behaviors and strategies
6. **Quality** - Data confidence and completeness

**Processing Characteristics**:
- Generated by Python functions in `precompute_professional.py`
- Processing time: 0.001s per analysis
- Cost: $0.00 (no API usage)
- 100% success rate with fail-fast architecture
- Output wrapped in metadata structure with `response` field containing stringified JSON

---

## Output Wrapper Structure

All analysis outputs are wrapped in this metadata structure:

```json
{
  "prompt_type": "analysis_type_name",
  "success": true,
  "response": "{...stringified 6-block JSON...}",
  "error": null,
  "processing_time": 0.001,
  "tokens_used": 0,
  "estimated_cost": 0.0,
  "retry_attempts": 0,
  "timestamp": "2025-08-07T18:42:30.857222"
}
```

The actual analysis data is in the `response` field as a **stringified JSON**.

---

## 1. Creative Density (VERIFIED ✅)

### Generator Function
`compute_creative_density_analysis()` in `precompute_professional.py`

### Actual Output Structure (from real output file)

```json
{
  "densityCoreMetrics": {
    "avgDensity": float,              // e.g., 13.1
    "maxDensity": float,              // e.g., 27.0
    "minDensity": float,              // e.g., 2.0
    "stdDeviation": float,            // e.g., 6.007
    "totalElements": int,             // e.g., 393
    "elementsPerSecond": float,       // e.g., 13.1
    "elementCounts": {
      "text": int,                    // OCR text overlays
      "sticker": int,                 // HSV-based sticker detection
      "effect": int,                  // Visual effects (usually 0)
      "transition": int,              // Scene transitions
      "object": int,                  // YOLO detections
      "gesture": int,                 // MediaPipe gestures
      "expression": int               // Facial expressions
    },
    "sceneChangeCount": int,          // Total scene changes
    "timelineCoverage": float,        // 0.0-1.0 coverage
    "confidence": float               // e.g., 0.95
  },
  
  "densityDynamics": {
    "densityCurve": [
      {
        "second": int,                // Time index
        "density": int,               // Elements at this second
        "primaryElement": string      // Dominant element type
      }
    ],
    "volatility": float,              // Density variation measure
    "accelerationPattern": string,    // "even" | "front_loaded" | "back_loaded"
    "densityProgression": string,     // "stable" | "increasing" | "decreasing"
    "emptySeconds": [int],            // Array of empty second indices
    "confidence": float               // e.g., 0.95
  },
  
  "densityInteractions": {
    "multiModalPeaks": [
      {
        "timestamp": string,          // e.g., "12-13s"
        "elements": [string],         // e.g., ["text", "object", "expression"]
        "syncType": string            // "complementary" | "reinforcing"
      }
    ],
    "elementCooccurrence": {
      "object_text": int,             // Co-occurrence counts
      "expression_object": int,
      "expression_text": int
      // Note: Dynamic keys based on actual co-occurrences
    },
    "dominantCombination": string,    // e.g., "object_text"
    "coordinationScore": float,       // 0.0-1.0
    "confidence": float               // e.g., 0.9
  },
  
  "densityKeyEvents": {
    "peakMoments": [
      {
        "timestamp": string,          // e.g., "11-12s"
        "totalElements": int,         // e.g., 27
        "surpriseScore": float,       // Statistical surprise measure
        "elementBreakdown": {
          "text": int,
          "sticker": int,
          "effect": int,
          "transition": int,
          "scene_change": int
        }
      }
    ],
    "deadZones": [                    // Empty or sparse periods
      {
        "start": int,
        "end": int,
        "duration": int
      }
    ],
    "densityShifts": [
      {
        "timestamp": int,             // Second of shift
        "from": string,               // "low" | "medium" | "high"
        "to": string,                 // "low" | "medium" | "high"
        "magnitude": float            // Shift magnitude
      }
    ],
    "confidence": float
  },
  
  "densityPatterns": {
    "structuralFlags": {
      "strongOpeningHook": boolean,
      "crescendoPattern": boolean,
      "frontLoaded": boolean,
      "consistentPacing": boolean,
      "finalCallToAction": boolean,
      "rhythmicPattern": boolean
    },
    "densityClassification": string,  // "sparse" | "moderate" | "dense"
    "pacingStyle": string,            // "even" | "burst_fade" | "oscillating"
    "cognitiveLoadCategory": string,  // "minimal" | "optimal" | "challenging"
    "mlTags": [string],               // e.g., ["density_computed", "avg_13.1"]
    "confidence": float
  },
  
  "densityQuality": {
    "dataCompleteness": float,        // 0.0-1.0
    "detectionReliability": {
      "textOverlay": float,
      "sticker": float,
      "effect": float,
      "transition": float,
      "sceneChange": float,
      "object": float,
      "gesture": float
    },
    "overallConfidence": float        // e.g., 0.9
  }
}
```

---

## 2. Visual Overlay Analysis (VERIFIED ✅)

### Generator Function
`compute_visual_overlay_analysis_professional()` in `precompute_professional.py`

### Actual Output Structure (from real output file)

```json
{
  "visualOverlayCoreMetrics": {
    "totalOverlays": int,             // Text + stickers combined
    "totalTextOverlays": int,         // Just text overlays (NEW - added to match implementation)
    "totalStickers": int,             // Just stickers (NEW - added to match implementation)
    "uniqueOverlayCount": int,        // Unique text count
    "overlayDensity": float,          // Overlays per second
    "uniqueOverlayRatio": float,      // Unique/total ratio
    "timeToFirstOverlay": float,      // Seconds to first overlay
    "avgOverlayDuration": float,      // Average display time
    "overlayFrequency": float,        // Overlays per minute
    "confidence": float               // e.g., 0.92
  },
  
  "visualOverlayDynamics": {
    "overlayProgression": [           // NOTE: Not "overlayTimeline"
      {
        "timestamp": string,          // e.g., "0-3s"
        "overlayCount": int,          // Count in window
        "density": float              // Normalized density
      }
    ],
    "rhythmConsistency": float,       // Rhythm regularity measure
    "overlayAcceleration": string,    // "stable" | "accelerating" | "decelerating"
    "temporalDistribution": string,   // "front_loaded" | "balanced"
    "burstPatterns": int,             // Count of rapid overlay sequences
    "confidence": float
  },
  
  "visualOverlayInteractions": {
    "overlaySpeechAlignment": float,  // NOTE: Not "textSpeechSync"
    "overlayGestureSync": float,      // Gesture coordination score
    "multimodalReinforcementCount": int,  // Count of multimodal moments
    "multimodalMoments": [
      {
        "timestamp": string,          // e.g., "0s"
        "textContent": string,        // Actual text shown
        "hasSpeech": boolean,         // Speech present
        "hasGesture": boolean         // Gesture present
      }
    ],
    "crossModalCoherence": float,     // Overall alignment score
    "confidence": float
  },
  
  "visualOverlayKeyEvents": {
    "overlayPeaks": [                 // High-density moments
      {
        "timestamp": string,          // e.g., "0-3s"
        "overlayCount": int,
        "intensity": float            // Normalized intensity
      }
    ],
    "ctaMoments": [],                 // Call-to-action detections (often empty)
    "climaxMoment": string,           // Peak overlay moment timestamp
    "quietMoments": [string],         // Periods with no overlays
    "confidence": float
  },
  
  "visualOverlayPatterns": {
    "overlayStrategy": string,        // "minimal" | "moderate" | "heavy"
    "overlayTechniques": [string],    // e.g., ["multimodal_reinforcement"]
    "engagementArchetype": string,    // "informational" | "emotional" | "promotional"
    "pacingPattern": string,          // "stable" | "building" | "variable"
    "contentFocus": string,           // "informational" | "entertainment"
    "confidence": float
  },
  
  "visualOverlayQuality": {
    "detectionConfidence": float,     // OCR confidence
    "dataCompleteness": float,        // Data coverage
    "analysisReliability": string,    // "high" | "medium" | "low"
    "missingDataPoints": [],          // List of missing data
    "timelineCoverage": float,        // Portion of video with overlays
    "overallConfidence": float
  }
}
```

---

## 3. Emotional Journey (VERIFIED ✅)

### Generator Function
`compute_emotional_journey_analysis_professional()` in `precompute_professional.py`

### Actual Output Structure

```json
{
  "emotionalCoreMetrics": {
    "uniqueEmotions": int,            // Count of different emotions
    "emotionTransitions": int,        // Number of emotion changes
    "dominantEmotion": string,        // Most frequent emotion
    "emotionalDiversity": float,      // Variety score (0-1)
    "gestureEmotionAlignment": float, // Gesture-emotion sync
    "audioEmotionAlignment": float,   // Audio-emotion match (usually 0.0)
    "captionSentiment": string,       // "neutral" (placeholder)
    "emotionalIntensity": float,      // Average confidence
    "confidence": float
  },
  
  "emotionalDynamics": {
    "emotionProgression": [
      {
        "timestamp": string,          // e.g., "0-6s"
        "emotion": string,            // Primary emotion
        "intensity": float            // Confidence/intensity
      }
    ],
    "transitionSmoothness": float,    // Gradual vs abrupt changes
    "emotionalArc": string,           // "dynamic" | "evolving" | "stable"
    "peakEmotionMoments": [
      {
        "timestamp": string,
        "emotion": string,
        "intensity": float
      }
    ],
    "stabilityScore": float,
    "tempoEmotionSync": float,        // Music-emotion alignment
    "confidence": float
  },
  
  "emotionalInteractions": {
    "gestureReinforcement": float,
    "audioMoodCongruence": float,
    "captionEmotionAlignment": float,
    "multimodalCoherence": float,
    "emotionalContrastMoments": [],   // Conflicting signals
    "confidence": float
  },
  
  "emotionalKeyEvents": {
    "emotionalPeaks": [
      {
        "timestamp": string,
        "emotion": string,
        "trigger": string
      }
    ],
    "transitionPoints": [
      {
        "timestamp": string,
        "from": string,
        "to": string,
        "trigger": string
      }
    ],
    "climaxMoment": string,           // Simple timestamp string
    "resolutionMoment": {
      "timestamp": string,
      "emotion": string,
      "closure": boolean
    },
    "confidence": float
  },
  
  "emotionalPatterns": {
    "journeyArchetype": string,       // "transformation" | "discovery" etc.
    "emotionalTechniques": [string],
    "pacingStrategy": string,
    "engagementHooks": [string],
    "viewerJourneyMap": string,
    "confidence": float
  },
  
  "emotionalQuality": {
    "detectionConfidence": float,
    "timelineCoverage": float,
    "emotionalDataCompleteness": float,
    "analysisReliability": string,
    "missingDataPoints": [string],
    "overallConfidence": float
  }
}
```

---

## 4. Person Framing, Scene Pacing, Speech Analysis, Metadata Analysis

These use wrapper functions that convert basic metrics to professional format. The structures in the original documentation are largely correct for these, with the standard 6-block format.

---

## Implementation Notes

### Actual File Locations
- **Generator Functions**: `/home/jorge/rumiaifinal/rumiai_v2/processors/precompute_professional.py`
- **Output Files**: `/home/jorge/rumiaifinal/insights/{video_id}/{analysis_type}/`
- **Wrapper**: All outputs wrapped with metadata including `prompt_type`, `success`, `response` (stringified), etc.

### Key Implementation Details

1. **String Response**: The actual 6-block JSON is stringified in the `response` field
2. **Dynamic Fields**: Some fields like `elementCooccurrence` have dynamic keys based on actual data
3. **Placeholder Values**: Some fields like `audioEmotionAlignment` are placeholders (0.0)
4. **Field Naming**: Mix of camelCase (most fields) and snake_case (some dynamic keys)
5. **Empty Arrays**: Many "KeyEvents" arrays are empty in practice (e.g., `ctaMoments`)

### Confidence Score Patterns
- **CoreMetrics**: Usually 0.85-0.95
- **Dynamics**: Usually 0.88-0.95
- **Interactions**: Usually 0.85-0.90
- **KeyEvents**: Usually 0.87-0.95
- **Patterns**: Usually 0.82-0.87
- **Quality**: Overall 0.90-0.95

### Data Sources
- **YOLO**: Object detection → `object` counts
- **OCR**: Text detection → `text` overlays
- **MediaPipe**: Pose/gesture → `gesture`, `expression`
- **Whisper**: Speech → speech timeline
- **Scene Detection**: Scene boundaries → `sceneChangeCount`

---

## Validation Status

✅ **Verified Against**:
- Actual implementation in `precompute_professional.py`
- Real output files from `/insights/` directory
- Field names cross-checked with code
- Data types confirmed from actual outputs

⚠️ **Known Discrepancies Fixed**:
1. `totalOverlays` vs `totalTextOverlays` confusion
2. `overlaySpeechAlignment` vs `textSpeechSync` naming
3. `overlayProgression` vs `overlayTimeline` naming
4. Missing fields that were documented but not implemented

This document now accurately reflects the ACTUAL Python-only output structures as implemented in RumiAI v2.